{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4igafQDQS1jz"
   },
   "source": [
    "\n",
    "<h1><b><center>Programming Assignment II : </center></b></h1>\n",
    "<h1><b><center>Metropolis Hastings Algorithm</center></b></h1>\n",
    "<h5><b><center> Amatya Sharma </center></b></h5>\n",
    "<h5><b><center> 17CS30042 </center></b></h5>\n",
    "<h5><b><center> Note : A separate PDF of a sample run has been uploaded </center></b></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNgC1_MrS1j1"
   },
   "source": [
    "# Objective\n",
    "## Input \n",
    "- $n$ := number of iterations of algorithm\n",
    "- $K$ := number of bivariate gaussians for the mixture distribution \n",
    "- $sigma$ := standard deviation for stepping normal distribution\n",
    "\n",
    "\n",
    "## Goal\n",
    "- Build a GMM with the given values of means and covariances\n",
    "- Draw n samples from the produced GMM using Metropolis-Hastings algorithm\n",
    "- Plot them in $2D$ \n",
    "\n",
    "\n",
    "## Output\n",
    "For building dataset we build and plot:\n",
    "- $K$ - bivariate gaussians with means as $[2i, 3i]$ and covariances as $0.5*sqrt(i)*I$ for all $1 <= i <= K$\n",
    "- Uniform weighted mixture model of all $K$ - bivariate gaussians\n",
    "\n",
    "Metropolis-Hastings algorithm :\n",
    "- Draw $n$ samples from the produced GMM\n",
    "- Plot the walk of sampled points along with rejected points\n",
    "- Plot the accept points on $2D$ as well as $1D$ for both the dimensions for accepted points to visualize the sampled distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8z3sBFWXNNC"
   },
   "source": [
    "## Dependencies and Notations in code part :\n",
    "- `n` := number of iterations of algorithm\n",
    "- `K` := number of bivariate gaussians for the mixture distribution \n",
    "- `sigma` := standard deviation for stepping normal distribution\n",
    "- `accepted` := points accepted by metropolis hastings algo\n",
    "- `rejected` := pts rejected by metropolis algo\n",
    "- `samples` := pts sampled by metroplois algo\n",
    "- `rv[i]` := ith Gaussian distribution\n",
    "- `mu[i]` := mean of `rv[i]`\n",
    "- `cov[i]` := covariance of `rv[i]`\n",
    "- `x1` := $1^{st}$ element of bivariate gaussian\n",
    "- `x2` := $2^{nd}$ element of bivariate gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2JI429fS1j2"
   },
   "source": [
    "## Metropolis-Hasting Algorithm\n",
    "\n",
    "Metropolis-Hastings algorithm is sampling algorithm to sample from high dimensional, which is otherwise difficult to sample directly (due to intractable integrals) distributions or functions.\n",
    "\n",
    "It employs Markov Chain because to get the next sample, one only need to consider the current sample and Monte Carlo as it generates random sample which we could use to compute integrals or numerical results.\n",
    "\n",
    "The core of the algorithm lies in the distribution $Q(x'|x)$, which is used to suggest the next candidate of the Markov Chain given the current state/sample, and the acceptance probability alpha which is used to decide whether we accept the new sample, or stay with the current sample.\n",
    "\n",
    "The acceptance probability alpha is found by this equation:\n",
    "    \\begin{equation}\n",
    "alpha = min(1, \\frac{P(x').Q(x|x')}{P(x).Q(x'|x)})\n",
    "    \\end{equation}\n",
    "where :\n",
    "- $P(x)$ is the target distribution = GMM\n",
    "- $Q$ is proposal distribution = Gaussian Distribution N(0, sigma)\n",
    "\n",
    "In case of gaussian, transition distribution $Q(x|y)$ is symmetric i.e. \n",
    "\\begin{equation}\n",
    "Q(x|x') = Q(x'|x)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\implies \\frac{Q(x|x')}{Q(x'|x)} = 1\n",
    "\\end{equation}\n",
    "    \\begin{equation}\n",
    "alpha = min(1, \\frac{P(x').Q(x|x')}{P(x).Q(x'|x)})\n",
    "    \\end{equation}\n",
    "        \\begin{equation}\n",
    "\\implies alpha = min(1, \\frac{P(x')}{P(x)})\n",
    "    \\end{equation}\n",
    "\n",
    "- We initialize the pts. by randomly sampling with a gaussian distribution with mean of first gaussian of the mizture and covariance s.t. it covers almost the entire distribution.\n",
    "\n",
    "### Implementation\n",
    "We use function ` def metropolis_hastings(p, n, sigma) ` defined as follows:\n",
    "\n",
    "```python\n",
    "def metropolis_hastings(p, n, sigma):\n",
    "    x, y = np.random.multivariate_normal([2,3], [[math.sqrt(1)/2,0],[0,math.sqrt(1)/2]]).T\n",
    "    samples = np.zeros((n,2))  # has all the walk points both rej and acc\n",
    "    accepted = []  # only new walk points\n",
    "    rejected = []  # if no change has happended, predicteed pts xstar and ystar go to rej\n",
    "    \n",
    "    for i in range(n): # perform for n iterations\n",
    "        x_star, y_star = np.array([x, y]) + np.random.normal(scale=(sigma), size = 2)\n",
    "        # sample a move such that d(x) ~ N(0,sigma)\n",
    "        u=np.random.uniform(0,1)\n",
    "        if u < p(x_star, y_star) / p(x, y): # accept and move to new pt\n",
    "            x, y = x_star, y_star\n",
    "            accepted.append(np.array([x, y]))\n",
    "        else :  # reject\n",
    "            rejected.append(np.array([x_star, y_star]))\n",
    "        samples[i] = np.array([x, y])\n",
    "    return np.array(accepted), np.array(rejected), np.array(samples)\n",
    "```\n",
    "\n",
    "The functions works as follows:\n",
    "- sample a move such that $d(x)$ ~ $N(0,sigma)$\n",
    "- check the proposal probability and consequently alpha\n",
    "- randomly sample a value from $uniform$ $[0,1]$ distribution and make a move if alpha is more than the sample value\n",
    "\n",
    "The function outputs :\n",
    "- `samples[]` : has all the walk points both rej and acc\n",
    "- `accepted[]` : only new walk points\n",
    "- `rejected[]` : if no change has happended, predicteed pts xstar and ystar go to rej\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQua_20gS1j3"
   },
   "source": [
    "## Building Dataset\n",
    "\n",
    "- We build $K$ bivariate gaussians with given means and covariances using library function `scipy.stats.multivariable_normal()`\n",
    "- Obtained GMM as a mean of all $K$ pdfs\n",
    "- Visualized both $K$-bivariate normal distributions and GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jLt5P4rS1j5"
   },
   "source": [
    "## Running of Algorithm\n",
    "- Build dataset as described\n",
    "- Obtain probability distribution functions for GMM using function `def mixture_GM_pdf(x1, x2)`\n",
    "- Obtain sampling using Metropolis-Hastings Algo as described using function `def metropolis_hastings(mixture_GM_pdf, n, sigma)`\n",
    "- Plot the data using `def visualize_data(rejected, samples, n, sigma, K)` function\n",
    "  - \n",
    "  ```python\n",
    "def visualize_data(rejected, samples, n, sigma, K):\n",
    "    print(\"Visualized Data on (n, sigma, K):\", n, sigma, K)\n",
    "    plt.plot(rejected[:,0], rejected[:,1], 'rx', label = \"Rejected\", color = \"red\")\n",
    "    plt.plot(samples[:,0], samples[:,1], label = \"Walk\")\n",
    "    plt.title(\"Walk of Metropolis Algorithm\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n First Direction Plot against no. of iters:\\n\")\n",
    "    iters=[i for i in range(1,n+1)]\n",
    "    #print(yt)\n",
    "    plt.plot(iters,samples[:,0])\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('%xdel1')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n First Direction Frequency Plot against no. of iters:\\n\")\n",
    "    plt.hist(samples[:,0],100)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('Frequency of samples')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Sampled Points:\\n\")\n",
    "    sns.jointplot(samples[:, 0], samples[:, 1])\n",
    "    return\n",
    "```\n",
    "  - Plot the accept points on $2D$ \n",
    "  - Plot $1D$ for both the dimensions for accepted points to visualize the sampled distribution\n",
    "  - Plot $1D$ frequency histogram for first dimension\n",
    "  - Plot first dimension sampled pts against iterations of algorithm\n",
    "\n",
    "\n",
    "```python\n",
    "def run_algo(n, K, sigma):\n",
    "  # prepare position coordinates \n",
    "  # these are just used to visualize data\n",
    "  # no use in forming distibutions\n",
    "  x, y = np.mgrid[0:3*K:.1, 0:4*K:.1]\n",
    "  pos = np.dstack((x,y))\n",
    "\n",
    "  # create k bivariate gaussians with required means and covariances\n",
    "  rv = {}\n",
    "  mx = np.zeros(np.shape(x))\n",
    "  i = 1\n",
    "  mu =[]\n",
    "  cov =[]\n",
    "  while i<=K:\n",
    "      mui = [2*i, 3*i]\n",
    "      sigmai = np.eye(2)*0.5*np.sqrt(i)\n",
    "      rv[i] = multivariate_normal(mui, sigmai)\n",
    "      mx = np.add( mx, rv[i].pdf(pos) )\n",
    "      plt.contour(x,y,rv[i].pdf(pos))\n",
    "      mu.append(mui)\n",
    "      cov.append(sigmai)\n",
    "      i+=1\n",
    "\n",
    "  def mixture_GM_pdf(x1, x2):\n",
    "    i = 1\n",
    "    mx = 0.0\n",
    "    while i<=K:\n",
    "      mx += rv[i].pdf([x1,x2])\n",
    "      i += 1\n",
    "    mx = mx/K\n",
    "    return mx\n",
    "\n",
    "  plt.title(\"Uniform Mixture Gaussian\")\n",
    "  plt.contour(x,y,mx)\n",
    "  plt.xlabel(\"x1\")\n",
    "  plt.ylabel(\"x2\")\n",
    "  plt.show()\n",
    "\n",
    "  accepted, rejected, samples = metropolis_hastings(mixture_GM_pdf, n, sigma)\n",
    "\n",
    "  visualize_data(rejected, samples, n, sigma, K)\n",
    "\n",
    "  return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUBQF5JjS1j8"
   },
   "source": [
    "## Conclusions\n",
    "- From the runs of algorithm we observe:\n",
    "  - Usually the algorithm performs poor for very small values of $K$\n",
    "  - For median values of $K$ (~$5-10$) the samples closely resembled the original distribution\n",
    "  - For very small values of $sigma$ $($ ~ $.01-1.5)$ the algorithm rejected a lot of samples and gave a poor outcome for median values of $K$ $($~$10-15)$\n",
    "  - Best approximations were observed at $sigma$ ~ $2.0-5.0$\n",
    "  - Clearly increasing $n$ is imporving results\n",
    "- Metropolis-Hastings has an advantage over other methods of sampling such as Gibbs Sampling as we don’t have to derive the conditional distributions analytically. We just need to know the joint distribution, and no need to derive anything analytically.\n",
    "- Gibbs Sampling is much faster as compared to Metropolis-Hastings as s0ampling from conditional distribution is really fast, whereas sampling from full joint distribution is slow.\n",
    "- While sampling the threshold value from uniform distribution, we can ignore the `min(1, x)` term for the alpha calculation, and just calculate `P(x') / P(x)`, because we only care about whether or not the ratio is bigger than some uniform random number `[0, 1]`, so when `P(x') / P(x) > 1`, it will then always satisfy the test just as `P(x') / P(x) = 1`, calculated from the `min(1, x)` term.\n",
    "- Initializing with gaussian distribution s.t. the mean lies around the center of the distribution we are approximating helps sampling better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjFM2PVdS1j9"
   },
   "source": [
    "## References\n",
    "$[1]$ Peter Driscoll, “A comparison of least-squares and Bayesian fitting techniques to radial velocity data sets” <br>\n",
    "$[2]$ Carson Chow, “MCMC and fitting models to data” <br>\n",
    "$[3]$ John H. Williamson, “Data Fundamentals - Probabilities” <br>\n",
    "$[4]$ Simon Rogers, “A first course in machine learning” <br>\n",
    "$[5]$ Agustinus Kristiadi's Blog on Metropolis-Hastings <br>\n",
    "$[6]$ Kevin P. Murphy. Machine Learning, A Probabilistic Perspective <br>\n",
    "$[7]$ Christopher M. Bishop. Pattern Recognition and Machine Learning <br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Report_Assgn2_AML_17CS30042.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
